{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Method Comparison\n",
    "\n",
    "Compare different GP training approaches focusing on:\n",
    "- Training time cost for small number of simulations in different data type (kSZ, deltaSigma)\n",
    "- Prediction accuracy (test_plot style) for same number of testing data\n",
    "- Save all plots to save_dir\n",
    "- Load pre-trained models for Methods 1-3 from trained_gp_models/GP_comparison_090625\n",
    "- Use conditional_gp training for Method 4 with different kernel types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained models from: trained_gp_models/GP_comparison_090625\n"
     ]
    }
   ],
   "source": [
    "filterType= 'dsigma' #'CAP' \n",
    "ptype= 'total' #'gas'\n",
    "n_sims = 20  # Number of simulations to train\n",
    "plot_mode = 'train'\n",
    "\n",
    "PRETRAIN_AVAILABLE = False\n",
    "# Path to pre-trained models\n",
    "pretrained_dir = \"trained_gp_models/GP_comparison_090625_CAP\"\n",
    "print(f\"Loading pre-trained models from: {pretrained_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/u1/l/lindajin/virtualenvs/env1/lib/python3.11/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.6.1 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: trained_gp_models/GP_comparison_090825_dsigma\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup save directory\n",
    "save_dir = f\"trained_gp_models/GP_comparison_{datetime.now().strftime('%m%d%y')}_{filterType}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Results will be saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0)]\n",
      "Using device: TFRT_CPU_0\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "Using device: TFRT_CPU_0\n",
      "✅ Improved trainer available\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import importlib\n",
    "import GP_dataloader\n",
    "importlib.reload(GP_dataloader) \n",
    "from GP_dataloader import *\n",
    "import train_GP\n",
    "importlib.reload(train_GP) \n",
    "from train_GP import *\n",
    "\n",
    "try:\n",
    "    from src.models.improved_kernels import *\n",
    "    IMPROVED_AVAILABLE = True\n",
    "    print(\"✅ Improved trainer available\")\n",
    "except ImportError:\n",
    "    IMPROVED_AVAILABLE = False\n",
    "    print(\"❌ Improved trainer not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 20/204 sims, Testing: 20 sims\n",
      "Getting total profiles with dsigma filter for 20 simulations...\n",
      "Finished getting profiles in 1360 halos.\n",
      "Profiles shape: (1360, 21), Mass shape: (1360,), Params shape: (1360, 35), PkRatio shape: (1360, 255)\n",
      "Train data [ 3 10 23 25 30 31 39 44 54 55 59 60 63 66 67 70 72 76 86 88]: X=(1360, 291), y=(1360, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "sim_indices_train = np.load('data/sparse_sampling_train_indices_random.npy')  \n",
    "sim_indices_test = np.load('data/sparse_sampling_test_indices_random.npy')[:n_sims]  # Use subset for testing\n",
    "sim_subset = sim_indices_train[:n_sims]  # Same subset used in original training\n",
    "\n",
    "print(f\"Training: {len(sim_subset)}/{len(sim_indices_train)} sims, Testing: {len(sim_indices_test)} sims\")\n",
    "\n",
    "# Load training data for predictions\n",
    "X_train, y_train, r_bins, k_bins = prepare_GP_data(sim_subset, filterType, ptype)\n",
    "print(f\"Train data {sim_subset}: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "# Load test data\n",
    "if plot_mode == 'test':\n",
    "    X_test, y_test, _, _ = prepare_GP_data(sim_indices_test, filterType, ptype)\n",
    "    print(f\"Test data {sim_indices_test}: X={X_test.shape}, y={y_test.shape}\")\n",
    "else:\n",
    "    X_test = X_train\n",
    "    y_test = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Load Pre-trained NN+GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Method 1: Loading Pre-trained NN+GP Model ===\n",
      "Getting total profiles with dsigma filter for 20 simulations...\n",
      "Finished getting profiles in 1360 halos.\n",
      "Profiles shape: (1360, 21), Mass shape: (1360,), Params shape: (1360, 35), PkRatio shape: (1360, 255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GP for each r_bin:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Adamw training for r_bin 0: Initial loss = 31578.4609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GP for each r_bin:   0%|          | 0/21 [02:01<?, ?it/s, Step=1600, Loss=5348.674316, Best=5348.674316] "
     ]
    }
   ],
   "source": [
    "print(\"=== Method 1: Loading Pre-trained NN+GP Model ===\")\n",
    "start_time = time.time()\n",
    "if PRETRAIN_AVAILABLE:\n",
    "    # Load pre-trained parameters\n",
    "    with open(os.path.join(pretrained_dir, \"best_params_nn_list.pkl\"), \"rb\") as f:\n",
    "        best_params_nn = pickle.load(f)\n",
    "    with open(os.path.join(pretrained_dir, \"model_info_nn_list.pkl\"), \"rb\") as f:\n",
    "        model_info_nn = pickle.load(f)\n",
    "else:\n",
    "    gp_models_nn, best_params_nn, model_info_nn = train_NN_gp(\n",
    "    sim_subset, filterType=filterType, ptype=ptype, save=False\n",
    ")\n",
    "\n",
    "nn_train_time = time.time() - start_time\n",
    "print(f\"NN+GP model loading time: {nn_train_time:.3f}s\")\n",
    "print(f\"Loaded {len(best_params_nn)} trained NN+GP models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NN+GP predictions\n",
    "print(\"Generating NN+GP predictions...\")\n",
    "pred_start = time.time()\n",
    "\n",
    "pred_means_nn = []\n",
    "pred_vars_nn = []\n",
    "model = build_NN_gp()\n",
    "\n",
    "for i in range(len(best_params_nn)):\n",
    "    cond_gp = model.apply(best_params_nn[i], X_train, y_train[:,i], X_test)[1]\n",
    "    pred_means_nn.append(cond_gp.mean)\n",
    "    pred_vars_nn.append(cond_gp.variance)\n",
    "\n",
    "pred_means_nn = np.array(pred_means_nn)\n",
    "pred_vars_nn = np.array(pred_vars_nn)\n",
    "nn_pred_time = time.time() - pred_start\n",
    "print(f\"NN+GP predictions shape: {pred_means_nn.shape} (generated in {nn_pred_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Load Pre-trained Hierarchical GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Method 2: Loading Pre-trained Hierarchical GP Model ===\")\n",
    "start_time = time.time()\n",
    "if PRETRAIN_AVAILABLE:\n",
    "    # Load pre-trained parameters\n",
    "    with open(os.path.join(pretrained_dir, \"best_params_hier_list.pkl\"), \"rb\") as f:\n",
    "        best_params_hier = pickle.load(f)\n",
    "    with open(os.path.join(pretrained_dir, \"model_info_hier_list.pkl\"), \"rb\") as f:\n",
    "        model_info_hier = pickle.load(f)\n",
    "else:\n",
    "    gp_models_hier, best_params_hier, model_info_hier = train_conditional_gp(\n",
    "    sim_subset, build_hierarchical_gp, maxiter=1000,\n",
    "    filterType=filterType, ptype=ptype, save=False\n",
    ")\n",
    "hier_train_time = time.time() - start_time\n",
    "print(f\"Hierarchical GP model loading time: {hier_train_time:.3f}s\")\n",
    "print(f\"Loaded {len(best_params_hier)} trained Hierarchical GP models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Hierarchical GP predictions\n",
    "print(\"Generating Hierarchical GP predictions...\")\n",
    "pred_start = time.time()\n",
    "\n",
    "# Rebuild GP models from saved parameters\n",
    "gp_models_hier = []\n",
    "for i in range(len(best_params_hier)):\n",
    "    gp_model = build_hierarchical_gp(best_params_hier[i], X_train)\n",
    "    gp_models_hier.append(gp_model)\n",
    "\n",
    "pred_means_hier = []\n",
    "pred_vars_hier = []\n",
    "\n",
    "for i, gp_model in enumerate(gp_models_hier):\n",
    "    _, cond_gp = gp_model.condition(y_train[:, i], X_test)\n",
    "    pred_means_hier.append(cond_gp.mean)\n",
    "    pred_vars_hier.append(cond_gp.variance)\n",
    "\n",
    "pred_means_hier = np.array(pred_means_hier)\n",
    "pred_vars_hier = np.array(pred_vars_hier)\n",
    "hier_pred_time = time.time() - pred_start\n",
    "print(f\"Hierarchical GP predictions shape: {pred_means_hier.shape} (generated in {hier_pred_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Training with Different Kernel Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.models.kernel_integration\n",
    "importlib.reload(src.models.kernel_integration)\n",
    "from src.models.kernel_integration import test_multiscale_training, test_physics_informed_training, test_robust_training\n",
    "\n",
    "if IMPROVED_AVAILABLE:\n",
    "    print(\"=== Method 4: Training with Different Kernels ===\")\n",
    "    \n",
    "    # Define kernel types to compare\n",
    "    kernel_types = ['multiscale', 'physics_informed', 'robust']\n",
    "    kernel_funcs = [test_robust_training, test_multiscale_training, test_physics_informed_training]\n",
    "    kernel_results = {}\n",
    "\n",
    "    for kernel_name, kernel_func in zip(kernel_types,kernel_funcs):\n",
    "        start_time = time.time()\n",
    "        if PRETRAIN_AVAILABLE:\n",
    "            print(f\"\\nLoading pretrained {kernel_name} kernel...\")\n",
    "            try:\n",
    "                # Load pretrained model\n",
    "                with open(os.path.join(pretrained_dir, f\"best_params_{kernel_name}_list.pkl\"), \"rb\") as f:\n",
    "                    best_params = pickle.load(f)\n",
    "                with open(os.path.join(pretrained_dir, f\"model_info_{kernel_name}_list.pkl\"), \"rb\") as f:\n",
    "                    model_info = pickle.load(f)\n",
    "                gp_models = [build_hierarchical_gp(best_params[i], X_train) for i in range(len(best_params))]\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Loading pretrained {kernel_name} kernel failed: {e}\")\n",
    "        else:\n",
    "            print(f\"\\nTraining with {kernel_name} kernel...\")\n",
    "            try:\n",
    "                # Train with current kernel\n",
    "                gp_models, best_params, model_info = kernel_func(\n",
    "                    sim_subset\n",
    "                )\n",
    "                with open(os.path.join(save_dir, f\"best_params_{kernel_name}_list.pkl\"), \"wb\") as f:\n",
    "                 pickle.dump(best_params, f)\n",
    "                with open(os.path.join(save_dir, f\"model_info_{kernel_name}_list.pkl\"), \"wb\") as f:\n",
    "                 pickle.dump(model_info, f)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {kernel_name} kernel failed: {e}\")\n",
    "                kernel_results[kernel_name] = {'error': str(e)}\n",
    "                \n",
    "        train_time = time.time() - start_time\n",
    "        print(f\"{kernel_name} kernel loading/training time: {train_time:.1f}s\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred_start = time.time()\n",
    "        pred_means = []\n",
    "        pred_vars = []\n",
    "        \n",
    "        for i, gp_model in enumerate(gp_models):\n",
    "            _, cond_gp = gp_model.condition(y_train[:, i], X_test)\n",
    "            pred_means.append(cond_gp.mean)\n",
    "            pred_vars.append(cond_gp.variance)\n",
    "        \n",
    "        pred_means = np.array(pred_means)\n",
    "        pred_vars = np.array(pred_vars)\n",
    "        pred_time = time.time() - pred_start\n",
    "        \n",
    "        kernel_results[kernel_name] = {\n",
    "            'train_time': train_time,\n",
    "            'pred_time': pred_time,\n",
    "            'pred_means': pred_means,\n",
    "            'pred_vars': pred_vars,\n",
    "            'models': gp_models,\n",
    "            'params': best_params\n",
    "        }\n",
    "        \n",
    "        print(f\"{kernel_name} predictions shape: {pred_means.shape} (generated in {pred_time:.2f}s)\")\n",
    "            \n",
    "\n",
    "    \n",
    "    print(f\"\\n✅ Completed training with {len([k for k, v in kernel_results.items() if 'error' not in v])} kernel types\")\n",
    "    \n",
    "else:\n",
    "    print(\"Method 4 skipped - improved trainer not available\")\n",
    "    kernel_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Loading Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all timing results\n",
    "def plot_time_comparison(timetype='train_time'):\n",
    "    methods = ['NN+GP', 'Hierarchical']\n",
    "    times = [eval(f'nn_{timetype}'), eval(f'hier_{timetype}')]\n",
    "    colors = ['blue', 'red']\n",
    "    \n",
    "    \n",
    "    # Add kernel training times\n",
    "    if kernel_results:\n",
    "        for kernel_name, results in kernel_results.items():\n",
    "            if 'error' not in results:\n",
    "                methods.append(f'{kernel_name}')\n",
    "                times.append(results[timetype])\n",
    "                colors.append(['orange', 'purple', 'brown', 'pink'][len(methods)-4])\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(methods, times, color=colors, alpha=0.7)\n",
    "    plt.ylabel('Time [s]', fontsize=14)\n",
    "    plt.title(f'GP {timetype} ({n_sims} sims) Time Comparison', fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(times)*0.01,\n",
    "                 f'{time_val:.3f}s' if time_val < 1 else f'{time_val:.1f}s', \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/{timetype}_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Time Summary:\")\n",
    "    for method, time_val in zip(methods, times):\n",
    "        print(f\"  {method}: {time_val:.3f}s\" if time_val < 1 else f\"  {method}: {time_val:.1f}s\")\n",
    "plot_time_comparison('train_time')\n",
    "# plot_time_comparison('pred_time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ground truth statistics\n",
    "upper = np.quantile(y_test, 0.25, axis=0)\n",
    "lower = np.quantile(y_test, 0.75, axis=0)\n",
    "median = np.median(y_test, axis=0)\n",
    "yerr_lower = np.abs(median - lower)\n",
    "yerr_upper = np.abs(upper - median)\n",
    "yerr_truth = [yerr_lower, yerr_upper]\n",
    "\n",
    "print(f\"Ground truth computed for {len(r_bins)} radius bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "n_plots = 2 + len([k for k, v in kernel_results.items() if 'error' not in v])\n",
    "fig, axes = plt.subplots(n_plots, 1, figsize=(12, 6*n_plots))\n",
    "if n_plots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# Helper function to plot method comparison\n",
    "def plot_method_comparison(ax, pred_means, pred_vars, method_name, color):\n",
    "    # Ground truth\n",
    "    ax.errorbar(r_bins, median, yerr=yerr_truth, fmt='o', capsize=5, capthick=2, \n",
    "                linewidth=2, markersize=6, color='black', label='Ground Truth')\n",
    "    \n",
    "    # Predictions\n",
    "    upper_pred = np.quantile(pred_means, 0.25, axis=1)\n",
    "    lower_pred = np.quantile(pred_means, 0.75, axis=1)\n",
    "    median_pred = np.mean(pred_means, axis=1)\n",
    "    yerr_lower_pred = np.abs(median_pred - lower_pred)\n",
    "    yerr_upper_pred = np.abs(upper_pred - median_pred)\n",
    "    yerr_pred = [yerr_lower_pred, yerr_upper_pred]\n",
    "    \n",
    "    ax.errorbar(r_bins, median_pred, yerr=yerr_pred, fmt='s', capsize=5, \n",
    "                capthick=2, linewidth=2, markersize=6, color=color, label=method_name)\n",
    "    ax.fill_between(r_bins, median_pred - np.mean(np.sqrt(pred_vars), axis=1), \n",
    "                    median_pred + np.mean(np.sqrt(pred_vars), axis=1), \n",
    "                    color=color, alpha=0.2, label=f'{method_name} 1σ')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Radius [Mpc/h]', fontsize=14)\n",
    "    ax.set_ylabel(r'CAP gas Profile [$M_\\odot$/h]', fontsize=14)\n",
    "    ax.set_title(f'{method_name} vs Ground Truth', fontsize=16)\n",
    "    \n",
    "    # Compute percentage error\n",
    "    percent_error = 100 * (median_pred - median) / median\n",
    "    return percent_error\n",
    "\n",
    "# Plot loaded models\n",
    "if plot_idx < len(axes):\n",
    "    error_nn = plot_method_comparison(axes[plot_idx], pred_means_nn, pred_vars_nn, \n",
    "                                     'NN+GP', 'blue')\n",
    "    plot_idx += 1\n",
    "\n",
    "if plot_idx < len(axes):\n",
    "    error_hier = plot_method_comparison(axes[plot_idx], pred_means_hier, pred_vars_hier, \n",
    "                                       'Hierarchical GP', 'red')\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot kernel comparisons\n",
    "kernel_colors = ['orange', 'purple', 'brown', 'pink']\n",
    "kernel_errors = {}\n",
    "\n",
    "color_idx = 0\n",
    "for kernel_name, results in kernel_results.items():\n",
    "    if 'error' not in results and plot_idx < len(axes):\n",
    "        kernel_errors[kernel_name] = plot_method_comparison(\n",
    "            axes[plot_idx], results['pred_means'], results['pred_vars'], \n",
    "            f'{kernel_name} GP', kernel_colors[color_idx % len(kernel_colors)]\n",
    "        )\n",
    "        plot_idx += 1\n",
    "        color_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{save_dir}/{plot_mode}_plot_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create percentage error comparison plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Helper function for percentage error\n",
    "def compute_percentage_error(pred_means, pred_vars, label, color, marker='o'):\n",
    "    median_pred = np.mean(pred_means, axis=1)\n",
    "    percent_error = 100 * (median_pred - median) / median\n",
    "    \n",
    "    plt.plot(r_bins, percent_error, marker=marker, linestyle='-', \n",
    "             color=color, linewidth=2, markersize=6, label=f'{label} Error')\n",
    "    \n",
    "    # Add uncertainty bands\n",
    "    uncertainty = np.mean(np.sqrt(pred_vars), axis=1)\n",
    "    error_upper = 100 * (median_pred + uncertainty - median) / median\n",
    "    error_lower = 100 * (median_pred - uncertainty - median) / median\n",
    "    \n",
    "    plt.fill_between(r_bins, error_lower, error_upper, color=color, alpha=0.2)\n",
    "    \n",
    "    return percent_error\n",
    "\n",
    "# Plot errors for all methods\n",
    "error_nn = compute_percentage_error(pred_means_nn, pred_vars_nn, 'NN+GP', 'blue', 's')\n",
    "error_hier = compute_percentage_error(pred_means_hier, pred_vars_hier, 'Hierarchical', 'red', '^')\n",
    "\n",
    "# Plot kernel errors\n",
    "markers = ['v', '<', '>', 'p']\n",
    "for i, (kernel_name, results) in enumerate(kernel_results.items()):\n",
    "    if 'error' not in results:\n",
    "        compute_percentage_error(results['pred_means'], results['pred_vars'], \n",
    "                               f'{kernel_name} GP', kernel_colors[i % len(kernel_colors)], \n",
    "                               markers[i % len(markers)])\n",
    "\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Radius [Mpc/h]', fontsize=14)\n",
    "plt.ylabel('Percentage Error [%]', fontsize=14)\n",
    "plt.ylim([-120,20])\n",
    "plt.title('GP Method Percentage Error Comparison', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{save_dir}/{plot_mode}_percentage_error_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary metrics\n",
    "def compute_summary_metrics(pred_means, pred_vars, ground_truth, method_name):\n",
    "    pred_median = np.mean(pred_means, axis=1)\n",
    "    \n",
    "    # Only use valid (non-NaN) data points\n",
    "    valid_mask = ~(np.isnan(pred_median) | np.isnan(ground_truth))\n",
    "    \n",
    "    if not np.any(valid_mask):\n",
    "        return f\"{method_name}: No valid predictions\"\n",
    "    \n",
    "    pred_valid = pred_median[valid_mask]\n",
    "    gt_valid = ground_truth[valid_mask]\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = np.mean((pred_valid - gt_valid)**2)\n",
    "    mae = np.mean(np.abs(pred_valid - gt_valid))\n",
    "    mape = np.mean(np.abs((pred_valid - gt_valid) / gt_valid)) * 100\n",
    "    \n",
    "    # High radius performance (last 5 bins)\n",
    "    high_r_mask = valid_mask[-5:]\n",
    "    if np.any(high_r_mask):\n",
    "        high_r_mape = np.mean(np.abs((pred_median[-5:][high_r_mask] - ground_truth[-5:][high_r_mask]) / \n",
    "                                   ground_truth[-5:][high_r_mask])) * 100\n",
    "    else:\n",
    "        high_r_mape = np.nan\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'high_radius_mape': high_r_mape,\n",
    "        'n_valid': np.sum(valid_mask)\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute metrics for all methods\n",
    "all_metrics = []\n",
    "all_metrics.append(compute_summary_metrics(pred_means_nn, pred_vars_nn, median, 'NN+GP (loaded)'))\n",
    "all_metrics.append(compute_summary_metrics(pred_means_hier, pred_vars_hier, median, 'Hierarchical (loaded)'))\n",
    "\n",
    "\n",
    "# Add kernel metrics\n",
    "for kernel_name, results in kernel_results.items():\n",
    "    if 'error' not in results:\n",
    "        all_metrics.append(compute_summary_metrics(results['pred_means'], results['pred_vars'], \n",
    "                                                  median, f'{kernel_name} GP'))\n",
    "\n",
    "print(f\"{'Method':<20} {'Valid':<5} {'MSE':<10} {'MAE':<10} {'MAPE%':<8} {'HighR%':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for metrics in all_metrics:\n",
    "    if isinstance(metrics, dict):\n",
    "        print(f\"{metrics['method']:<20} {metrics['n_valid']:<5} \"\n",
    "              f\"{metrics['mse']:<10.2e} {metrics['mae']:<10.2e} \"\n",
    "              f\"{metrics['mape']:<8.1f} {metrics['high_radius_mape']:<8.1f}\")\n",
    "    else:\n",
    "        print(metrics)\n",
    "\n",
    "print(f\"\\nTime Summary:\")\n",
    "for method, time_val in zip(methods, times):\n",
    "    print(f\"  {method}: {time_val:.3f}s\" if time_val < 1 else f\"  {method}: {time_val:.1f}s\")\n",
    "\n",
    "print(f\"\\n📁 All plots saved to: {save_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive summary data\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'loading_training_times': dict(zip(methods, times)),\n",
    "    'kernel_comparison_results': {\n",
    "        kernel: {'train_time': results.get('train_time', None), \n",
    "                'pred_time': results.get('pred_time', None),\n",
    "                'error': results.get('error', None)}\n",
    "        for kernel, results in kernel_results.items()\n",
    "    },\n",
    "    'metrics': {\n",
    "        metrics['method'].replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_').lower(): \n",
    "        metrics if isinstance(metrics, dict) else {'error': str(metrics)}\n",
    "        for metrics in all_metrics\n",
    "    },\n",
    "    'data_info': {\n",
    "        'n_train_sims': len(sim_subset),\n",
    "        'n_test_sims': len(sim_indices_test),\n",
    "        'n_radius_bins': len(r_bins),\n",
    "        'filter_type': 'CAP',\n",
    "        'particle_type': 'gas',\n",
    "        'pretrained_models_from': pretrained_dir\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{save_dir}/comparison_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Comprehensive summary saved to: {save_dir}/{plot_mode}_comparison_summary.json\")\n",
    "print(\"\\n✅ GP method comparison with different kernels complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
