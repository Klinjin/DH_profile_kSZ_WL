/global/u1/l/lindajin/virtualenvs/env1/lib/python3.11/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.6.1 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.
  warnings.warn(
üåå Physics-Informed Neural Network for Cosmological Halo Profiles
======================================================================
Demonstration of scalable, physics-constrained neural emulation
for cosmological parameter inference from LRG-like galaxy observations
======================================================================
üî¨ Physics-Informed Neural Network vs GP Scalability
============================================================
Dataset Size    GP Training     NN Training     Advantage
------------------------------------------------------------
100             Impractical     0.6h            Only viable option
500             Impractical     3.0h            Only viable option
1000            Impractical     6.0h            Only viable option

üí° Key Insight:
   Physics-informed NN maintains GP-level physics constraints
   while achieving neural network scalability and training speed

üî¨ Physics Constraints in Neural Network Architecture
============================================================
‚úÖ Same Physics Knowledge as GP Kernels:
   ‚Ä¢ Mass-radius scaling relationships (NFW profiles, virial scaling)
   ‚Ä¢ Cosmological parameter importance weighting (attention mechanism)
   ‚Ä¢ Power spectrum suppression effects (baryonic feedback)
   ‚Ä¢ Radial profile smoothness constraints (physics regularization)
   ‚Ä¢ Uncertainty quantification (deep ensemble ‚âà GP posterior)

üèóÔ∏è  Architecture Components:
   ‚Ä¢ CosmologyAttention: Weights different cosmological parameters by importance
   ‚Ä¢ MassScalingLayer: Enforces known mass-concentration relations
   ‚Ä¢ PowerSpectrumProcessor: Incorporates baryonic suppression physics
   ‚Ä¢ PhysicsRegularization: Mass scaling + smoothness + monotonic constraints

üéØ Physics Loss Components:
   ‚Ä¢ Mass scaling consistency (profiles scale correctly with halo mass)
   ‚Ä¢ Radial smoothness (avoid spurious oscillations)
   ‚Ä¢ Physics consistency (reasonable profile shapes)
   ‚Ä¢ Ensemble diversity (encourage uncertainty quantification)

‚öñÔ∏è  Not Just a Black Box:
   ‚Ä¢ Interpretable attention weights for cosmological parameters
   ‚Ä¢ Physically meaningful intermediate representations
   ‚Ä¢ Constrained parameter spaces (learnable but bounded)
   ‚Ä¢ Physics-informed loss functions guide training

üîÑ Workflow Comparison: Physics NN vs GP (Based on GP_comparison_090625_CAP)
======================================================================
üìä GP Performance - Training vs Testing (Severe Overfitting Detected):
-------------------------------------------------------------------------------------
Method               Train Time   Train MAPE%  Test MAPE%   Overfitting 
-------------------------------------------------------------------------------------
Hierarchical GP      42.5min      29.1         100.0        3.4x        
Robust GP            70.1min      31.6         100.0        3.2x        
Physics-Informed GP  54.5min      32.3         100.0        3.1x        
Multiscale GP        61.3min      51.9         80.7         1.6x        
NN+GP (hybrid)       51.4min      41.9         92.0         2.2x        

üéØ Physics NN Goals (Apple-to-Apple Test Comparison):
   ‚Ä¢ Target test accuracy: ‚â§80.7% MAPE (match best GP: Multiscale GP)
   ‚Ä¢ Avoid overfitting: Test MAPE should be close to training MAPE
   ‚Ä¢ Scalability: Process 1000+ simulations (vs GP's 20)
   ‚Ä¢ Test dataset: Same 20 test simulations as GP benchmark
   ‚Ä¢ Efficiency: Measured via actual training time per sample

‚ö†Ô∏è  Efficiency Assessment:
   ‚Ä¢ All performance claims will be validated through direct measurement
   ‚Ä¢ Training efficiency calculated as: (samples/minute)
   ‚Ä¢ Overall improvement: (accuracy √ó efficiency) comparison

GP (Current Reality):
   1. Load subset data (20 sims due to O(n¬≤) memory)
   2. Train best kernel (Hierarchical: 42.5min)
   3. SEVERE OVERFITTING: 29.1% train ‚Üí 100.0% test MAPE
   4. Best test: Multiscale GP at 80.6% MAPE
   5. Limited scalability and generalization issues

Physics NN (Target):
   1. Load full dataset (1000+ sims, O(n) scaling)
   2. Configure same physics constraints as GP kernels
   3. Train with regularization to prevent overfitting
   4. Target: <80.6% test MAPE (beat best GP) + good generalization
   5. Ready for large-scale NPE if overfitting solved

üî¨ Scientific Validation Requirements:
   ‚Ä¢ Match GP physics constraints: mass scaling, cosmology attention, PK suppression
   ‚Ä¢ Uncertainty quantification: ensemble ‚âà GP posterior
   ‚Ä¢ Same test methodology: CAP filter, gas particles, 21 radius bins
   ‚Ä¢ Performance threshold: MAPE ‚â§ 35% (within 20% of best GP)
   ‚Ä¢ Scalability demonstration: >1000 simulations successfully processed

ü§î Training Options:
   1. Quick demo (50 sims, ~10-20 min)
   2. CPU production (200+ sims, ~2-4 hours, optimized for HPC)
   3. GPU accelerated (200+ sims, ~1-2 hours, if available)
   4. Skip training (just show analysis)

Select option (1/2/3/4): Traceback (most recent call last):
  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/physics_neural_demo.py", line 623, in <module>
    trainer, results, efficiency_results = main()
                                           ^^^^^^
  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/physics_neural_demo.py", line 507, in main
    choice = input("\nSelect option (1/2/3/4): ").strip()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
