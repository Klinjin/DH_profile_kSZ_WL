/global/u1/l/lindajin/virtualenvs/env1/lib/python3.11/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.6.1 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.  warnings.warn(ðŸŒŒ Physics-Informed Neural Network Training==================================================ðŸ“Š GP Benchmark: 80.6% test MAPE (severe overfitting from 29% training)ðŸŽ¯ NN Target: <80% test MAPE + good generalizationðŸš€ Physics-Informed Neural Network Training Demo============================================================ðŸ“Š Demo Configuration:   â€¢ Simulations: 50 (scales to 1000+ for production)   â€¢ Filter: CAP (kinetic SZ)   â€¢ Particle: gas   â€¢ Expected training time: 10-20 minutes (scales to 2-8 hours)   â€¢ Physics constraints: Enabled (mass scaling, cosmology attention, PK suppression)ðŸ§  Model Architecture:   â€¢ Physics-informed components: True, True, True   â€¢ Hidden layers: [256, 128, 64]   â€¢ Uncertainty method: ensemble (size 3)   â€¢ Physics loss weights: 0.100Loading data for 50 simulations...Getting mean gas profiles with CAP filter for 50 simulations...Finished getting mean profiles from 50 simulations.Output shape: (50, 21) vs individual halo shape would be (n_halos*50, 21)Raw data loaded:  - Profiles: (50, 21)  - Masses: (50,)  - Params: (50, 35)  - PkRatio: (50, 79)Applying data transformations...Log mass range: [13.03, 14.22]âœ… Features normalized (zero mean, unit variance)Final data shapes:  - Features: (50, 115)  - Targets: (50, 21)Creating train/validation/test splits...Split created:  - Train: 40 samples (80.0%)  - Val:   5 samples (10.0%)  - Test:  5 samples (10.0%)âœ… DataLoader initialized:  - Dataset size: 50 samples, 115 features  - Train/Val/Test: 40/5/5  - Batch size: 32ðŸ“Š Dataset Information:   â€¢ Total samples: 50   â€¢ Features: 115 (35 cosmo + 1 mass + 79 PK)   â€¢ Target radius bins: 21   â€¢ Train/Val/Test: 40/5/5ðŸ–¥ï¸  JAX Device Configuration:   â€¢ Available devices: 1   â€¢ Default device: TFRT_CPU_0   â€¢ GPU available: False   ðŸ’» CPU training (consider GPU for faster training)ðŸƒ Starting Training...   â€¢ Physics constraints will guide the training process   â€¢ Uncertainty estimation via deep ensemble   â€¢ Early stopping based on validation loss   â€¢ Results saved to: /pscratch/sd/l/lindajin/DH_profile_kSZ_WL/trained_gp_models/physics_nn_20250910_213034ðŸ”¬ Training Physics-Informed Neural Network  - Dataset: 50 samples  - Features: 115  - Targets: 21 radius bins  - Physics constraints: Mass scaling, Cosmology attention, PK suppression  - Training config: 500 epochs, 50 patienceðŸš€ Model initialized:  - Architecture: Physics-informed ensemble  - Physics constraints: True, True, True  - Uncertainty method: ensemble  - Ensemble size: 3Training:   0%|                                                                                                          | 0/500 [00:00<?, ?it/s]Training:   0%|                                                                                                          | 0/500 [00:02<?, ?it/s]Traceback (most recent call last):  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/physics_neural_demo.py", line 627, in <module>    trainer, results, efficiency_results = main('demo')                                           ^^^^^^^^^^^^  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/physics_neural_demo.py", line 520, in main    trainer, results, efficiency_results = run_physics_neural_training_demo()                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/physics_neural_demo.py", line 338, in run_physics_neural_training_demo    results = trainer.train(              ^^^^^^^^^^^^^^  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/../src/models/physics_neural_trainer.py", line 341, in train    self.params, self.opt_state, train_loss, loss_dict = self.train_step(                                                         ^^^^^^^^^^^^^^^^  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/../src/models/physics_neural_trainer.py", line 251, in train_step    (loss_val, loss_dict), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File "/pscratch/sd/l/lindajin/DH_profile_kSZ_WL/examples/../src/models/physics_neural_trainer.py", line 231, in loss_fn    prediction_loss = jnp.mean((pred_mean - batch_y)**2)                                ~~~~~~~~~~^~~~~~~~~  File "/global/u1/l/lindajin/virtualenvs/env1/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py", line 589, in deferring_binary_op    raise TypeError(f"unsupported operand type(s) for {opchar}: "TypeError: unsupported operand type(s) for -: 'tuple' and 'ArrayImpl'--------------------For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.